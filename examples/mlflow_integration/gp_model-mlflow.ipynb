{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf3265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from lume_model.variables import ScalarVariable, DistributionVariable\n",
    "from lume_model.models.gp_model import GPModel\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# run below for local tracking: (see https://mlflow.org/docs/latest/getting-started/intro-quickstart/)\n",
    "# use whatever port is not being used\n",
    "# > mlflow server --host 127.0.0.1 --port 8082 --gunicorn-opts \"--timeout=60\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = (\n",
    "    \"http://127.0.0.1:8082\"  # or whatever port you use above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b1b10-5acd-4e26-8b74-2d243e0ce0be",
   "metadata": {},
   "source": [
    "# Multi-output example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53eab389-be6f-48a9-9521-c01a0c17a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# Create training data, 1 input, 3 outputs\n",
    "train_x = torch.rand(5, 1)\n",
    "train_y = torch.stack(\n",
    "    (\n",
    "        torch.sin(train_x * (2 * torch.pi)) + 0.1 * torch.randn(1),\n",
    "        torch.cos(train_x * (2 * torch.pi)) + 0.1 * torch.randn(1),\n",
    "        torch.sin(2 * train_x * (2 * torch.pi)) + 0.1 * torch.randn(1),\n",
    "    ),\n",
    "    dim=-1,\n",
    ").squeeze(1)\n",
    "\n",
    "\n",
    "# Initialize the GP model\n",
    "rbf_kernel = ScaleKernel(RBFKernel())\n",
    "\n",
    "model = SingleTaskGP(\n",
    "    train_x.to(dtype=torch.double),\n",
    "    train_y.to(dtype=torch.double),\n",
    "    covar_module=rbf_kernel,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "fit_gpytorch_mll(mll)\n",
    "\n",
    "# Derive posterior mean and variance\n",
    "model.eval()\n",
    "\n",
    "test_x = torch.rand(4, 10, 1)\n",
    "posterior = model.posterior(test_x)\n",
    "\n",
    "# Derive the posterior mean and variance for each output\n",
    "mean = posterior.mean\n",
    "variance = posterior.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b316363-a154-4ba8-b96b-30a22e400a35",
   "metadata": {},
   "source": [
    "## LUME-Model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746f7554-d317-400c-9e9b-47cb38422e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input variables\n",
    "input_variables = [ScalarVariable(name=\"x\")]\n",
    "\n",
    "# Define output variables\n",
    "# Currently the \"distribution_type\" field doesn't do anything\n",
    "output_variables = [\n",
    "    DistributionVariable(name=\"output1\"),\n",
    "    DistributionVariable(name=\"output2\"),\n",
    "    DistributionVariable(name=\"output3\"),\n",
    "]\n",
    "\n",
    "# Create lume_model instance\n",
    "gp_lume_model = GPModel(\n",
    "    model=model, input_variables=input_variables, output_variables=output_variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebbfdf-6bc4-4eae-a225-cce92a0e80e7",
   "metadata": {},
   "source": [
    "### Evaluate model and run methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b23c0a4-bf07-4351-b6e7-0762346f6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\"x\": test_x.to(dtype=torch.double).squeeze(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2c9bb9-d69a-4d1a-8f40-289aec891574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function returns a dictionary mapping each output to a torch.distributions.Distribution\n",
    "output_dict = gp_lume_model.evaluate(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c399e-bc7c-4872-a79b-eb666fc57345",
   "metadata": {},
   "source": [
    "# Register model to MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868d88a-703a-4515-9610-d4be2716463e",
   "metadata": {},
   "source": [
    "See function signature for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249c15b2-99ca-4825-81c5-fe1426a0690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mgp_lume_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_to_mlflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0martifact_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mregistered_model_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mversion_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlog_model_dump\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_jit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Registers the model to MLflow if mlflow is installed. Each time this function is called, a new version\n",
       "of the model is created. The model is saved to the tracking server or local directory, depending on the\n",
       "MLFLOW_TRACKING_URI.\n",
       "\n",
       "If no tracking server is set up, data and artifacts are saved directly under your current directory. To set up\n",
       "a tracking server, set the environment variable MLFLOW_TRACKING_URI, e.g. a local port/path. See\n",
       "https://mlflow.org/docs/latest/getting-started/intro-quickstart/ for more info.\n",
       "\n",
       "Args:\n",
       "    input_dict: Input dictionary to infer the model signature.\n",
       "    artifact_path: Path to store the model in MLflow.\n",
       "    registered_model_name: Name of the registered model in MLflow. Optional.\n",
       "    tags: Tags to add to the MLflow model. Optional.\n",
       "    version_tags: Tags to add to this MLflow model version. Optional.\n",
       "    alias: Alias to add to this MLflow model version. Optional.\n",
       "    run_name: Name of the MLflow run. Optional.\n",
       "    log_model_dump: Whether to log the model dump files as artifacts. Optional.\n",
       "    save_jit: Whether to save the model as TorchScript when calling model.dump, if log_model_dump=True. Optional.\n",
       "    **kwargs: Additional arguments for mlflow.pyfunc.log_model.\n",
       "\n",
       "Returns:\n",
       "    Model info metadata, mlflow.models.model.ModelInfo.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/SLAC/lume-model/lume_model/base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gp_lume_model.register_to_mlflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df00bb12-6920-4ba2-84d4-a68033010fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run lume-test at: http://127.0.0.1:8082/#/experiments/0/runs/5eaf5a2a90434e1bad7b897205f841ad\n",
      "üß™ View experiment at: http://127.0.0.1:8082/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'lume-model-multi-output-gp'.\n",
      "Created version '1' of model 'lume-model-multi-output-gp'.\n"
     ]
    }
   ],
   "source": [
    "model_info = gp_lume_model.register_to_mlflow(\n",
    "    input_dict=input_dict,\n",
    "    artifact_path=\"lume-model-multi-output-gp\",\n",
    "    registered_model_name=\"lume-model-multi-output-gp\",  # not always necessary but required for adding tags/aliases\n",
    "    tags={\"type\": \"test\"},  # example, if desired\n",
    "    version_tags={\"status\": \"deploy\"},  # example, if desired\n",
    "    alias=\"latest-gp\",  # example, if desired\n",
    "    run_name=\"lume-test\",  # will be generated randomly if not provided\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd607aad-224b-4b11-92a5-26e07a399f92",
   "metadata": {},
   "source": [
    "When calling `gp_lume_model.register_to_mlflow` again with the same `registered_model_name`, the model version will be incremented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1afc3f-2ad1-4f78-adef-bc80bda0cfa5",
   "metadata": {},
   "source": [
    "# Predict using loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943a5ed-8615-4154-b899-0608008187cc",
   "metadata": {},
   "source": [
    " Note that currently the loaded model requires a `dict[str, np.ndarray]`, and does not support tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be83f4e-76e8-4878-9a68-e048dcd15928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output1': MultivariateNormal(loc: torch.Size([4, 10]), covariance_matrix: torch.Size([4, 10, 10])),\n",
       " 'output2': MultivariateNormal(loc: torch.Size([4, 10]), covariance_matrix: torch.Size([4, 10, 10])),\n",
       " 'output3': MultivariateNormal(loc: torch.Size([4, 10]), covariance_matrix: torch.Size([4, 10, 10]))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict_np = {k: v.numpy() for k, v in input_dict.items()}\n",
    "version = model_info.registered_model_version\n",
    "gp_model_saved = f\"models:/lume-model-multi-output-gp/{version}\"\n",
    "gp_model_saved = mlflow.pyfunc.load_model(gp_model_saved)\n",
    "prediction = gp_model_saved.predict(input_dict_np)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec7b437e-28e3-48b1-a99c-eae43bae9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that outputs match\n",
    "assert (output_dict[\"output1\"].mean[0] == prediction[\"output1\"].mean[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fd513-d1cf-4648-8a4c-ebd6657ce0bb",
   "metadata": {},
   "source": [
    "# Logging other metrics/artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc777c-ac60-4241-ba24-80d5926dccc2",
   "metadata": {},
   "source": [
    "Note that the `lume_model.register_to_mlflow` ends the run automatically, but if you'd like to go back and update it, e.g. log an artifact, you can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54cbf72-cf25-48a7-9e51-a9e41faf593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run lume-test at: http://127.0.0.1:8082/#/experiments/0/runs/5eaf5a2a90434e1bad7b897205f841ad\n",
      "üß™ View experiment at: http://127.0.0.1:8082/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "run_id = model_info.run_id\n",
    "with mlflow.start_run(run_id=run_id) as run:\n",
    "    # log some metric\n",
    "    mlflow.log_metric(\"random_number\", 5)\n",
    "    # log some local file\n",
    "    mlflow.log_artifact(\"./gp_model-mlflow.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11dc1b-334d-4426-b435-3924c94d341b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
